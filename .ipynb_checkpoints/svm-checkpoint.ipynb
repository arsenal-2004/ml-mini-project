{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2223: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2225: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2233: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] /= 2  # halve the component at -N/2\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2234: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  temp = Y[sl]\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2236: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = temp  # set that equal to the component at -N/2\n"
     ]
    }
   ],
   "source": [
    "X, y, class_names = preprocessing.create_data_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X: (2565, 22, 57)\n",
      "Possible Classes: dict_values(['make', 'polite', 'draw', 'soon', 'money', 'cost', 'when', 'innocent', 'pen', 'name', 'know', 'paper', 'no', 'I', 'tray', 'research', 'computer_PC_', 'ready', 'God', 'what', 'wait_notyet_', 'building', 'yes', 'different', 'sad', 'man', 'right', 'later', 'all', 'hurry', 'his_hers', 'hear', 'danger', 'eat', 'drink', 'share', 'thank', 'you', 'temper', 'juice', 'hurt', 'wild', 'please', 'give', 'come', 'glove', 'forget', 'more', 'which', 'shop', 'lose', 'maybe', 'stubborn', 'question', 'where', 'sorry', 'spend', 'girl', 'Norway', 'write', 'science', 'zero', 'buy', 'happy', 'hot', 'not', 'take', 'will', 'head', 'go', 'is_true_', 'think', 'why', 'deaf', 'answer', 'surprise', 'how', 'read', 'love', 'flash', 'boy', 'voluntary', 'hello', 'cold', 'change_mind_', 'mine', 'crazy', 'responsible', 'who', 'joke', 'same', 'wrong', 'alive', 'us', 'exit'])\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of X:', X.shape)\n",
    "print('Possible Classes:', class_names.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X after flattening:  (2565, 1254)\n"
     ]
    }
   ],
   "source": [
    "X_flat = preprocessing.flatten_data(X)\n",
    "print('Dimensions of X after flattening: ', X_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions:  (1795, 1254)\n",
      "Test set dimensions:  (770, 1254)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.3)\n",
    "print('Training set dimensions: ', X_train.shape)\n",
    "print('Test set dimensions: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.424 (+/-0.015) for {'C': 0.01, 'kernel': 'linear'}\n",
      "0.888 (+/-0.039) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.931 (+/-0.011) for {'C': 1, 'kernel': 'linear'}\n",
      "0.930 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      0.70      0.82        10\n",
      "           3       1.00      0.33      0.50         3\n",
      "           4       1.00      0.91      0.95        11\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.88      1.00      0.93         7\n",
      "           7       0.75      0.86      0.80         7\n",
      "           8       0.71      0.71      0.71         7\n",
      "           9       0.82      1.00      0.90         9\n",
      "          10       1.00      0.75      0.86         8\n",
      "          11       0.89      1.00      0.94         8\n",
      "          12       1.00      0.80      0.89         5\n",
      "          13       0.92      0.92      0.92        12\n",
      "          14       0.90      0.90      0.90        10\n",
      "          15       1.00      0.83      0.91         6\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         8\n",
      "          18       1.00      1.00      1.00         6\n",
      "          19       0.86      1.00      0.92         6\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         8\n",
      "          22       1.00      0.83      0.91         6\n",
      "          23       1.00      1.00      1.00         9\n",
      "          24       1.00      0.90      0.95        10\n",
      "          25       1.00      0.71      0.83         7\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       1.00      1.00      1.00         7\n",
      "          28       1.00      1.00      1.00         9\n",
      "          29       0.88      1.00      0.93         7\n",
      "          30       1.00      1.00      1.00        10\n",
      "          31       1.00      1.00      1.00        15\n",
      "          32       1.00      1.00      1.00         8\n",
      "          33       0.90      1.00      0.95         9\n",
      "          34       1.00      1.00      1.00         8\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      0.83      0.91        12\n",
      "          37       1.00      1.00      1.00         6\n",
      "          38       1.00      1.00      1.00         7\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       0.89      0.89      0.89         9\n",
      "          41       0.90      1.00      0.95         9\n",
      "          42       1.00      1.00      1.00         8\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       0.89      1.00      0.94         8\n",
      "          45       0.80      0.89      0.84         9\n",
      "          46       1.00      1.00      1.00         8\n",
      "          47       1.00      1.00      1.00         7\n",
      "          48       1.00      1.00      1.00         9\n",
      "          49       1.00      0.92      0.96        12\n",
      "          50       1.00      1.00      1.00        10\n",
      "          51       0.90      0.90      0.90        10\n",
      "          52       1.00      1.00      1.00         9\n",
      "          53       0.92      0.92      0.92        12\n",
      "          54       0.92      1.00      0.96        11\n",
      "          55       1.00      0.86      0.92         7\n",
      "          56       1.00      1.00      1.00         6\n",
      "          57       0.75      0.75      0.75         4\n",
      "          58       1.00      0.92      0.96        12\n",
      "          59       1.00      1.00      1.00         7\n",
      "          60       1.00      0.92      0.96        12\n",
      "          61       0.50      1.00      0.67         3\n",
      "          62       0.89      1.00      0.94         8\n",
      "          63       1.00      1.00      1.00         7\n",
      "          64       0.60      0.86      0.71         7\n",
      "          65       1.00      1.00      1.00        11\n",
      "          66       1.00      1.00      1.00         9\n",
      "          67       0.89      1.00      0.94         8\n",
      "          68       1.00      1.00      1.00        10\n",
      "          69       1.00      1.00      1.00         9\n",
      "          70       1.00      0.90      0.95        10\n",
      "          71       0.71      0.56      0.63         9\n",
      "          72       1.00      1.00      1.00         5\n",
      "          73       1.00      0.86      0.92         7\n",
      "          74       0.88      1.00      0.93         7\n",
      "          75       1.00      1.00      1.00         7\n",
      "          76       1.00      0.86      0.92         7\n",
      "          77       0.75      1.00      0.86         9\n",
      "          78       1.00      1.00      1.00         8\n",
      "          79       1.00      1.00      1.00         5\n",
      "          80       0.88      0.88      0.88         8\n",
      "          81       1.00      0.91      0.95        11\n",
      "          82       1.00      1.00      1.00         6\n",
      "          83       1.00      0.83      0.91         6\n",
      "          84       1.00      0.89      0.94         9\n",
      "          85       0.89      1.00      0.94         8\n",
      "          86       0.70      0.78      0.74         9\n",
      "          87       1.00      1.00      1.00         5\n",
      "          88       0.86      1.00      0.92         6\n",
      "          89       1.00      1.00      1.00        10\n",
      "          90       0.86      0.86      0.86         7\n",
      "          91       1.00      1.00      1.00         6\n",
      "          92       1.00      1.00      1.00         7\n",
      "          93       0.80      1.00      0.89         4\n",
      "          94       1.00      1.00      1.00        15\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       770\n",
      "   macro avg       0.94      0.94      0.94       770\n",
      "weighted avg       0.95      0.94      0.94       770\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.451 (+/-0.027) for {'C': 0.01, 'kernel': 'linear'}\n",
      "0.875 (+/-0.036) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.918 (+/-0.007) for {'C': 1, 'kernel': 'linear'}\n",
      "0.919 (+/-0.008) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      0.70      0.82        10\n",
      "           3       1.00      0.67      0.80         3\n",
      "           4       1.00      0.91      0.95        11\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.88      1.00      0.93         7\n",
      "           7       0.75      0.86      0.80         7\n",
      "           8       0.71      0.71      0.71         7\n",
      "           9       0.82      1.00      0.90         9\n",
      "          10       1.00      0.75      0.86         8\n",
      "          11       0.89      1.00      0.94         8\n",
      "          12       1.00      0.80      0.89         5\n",
      "          13       0.92      0.92      0.92        12\n",
      "          14       0.90      0.90      0.90        10\n",
      "          15       1.00      0.83      0.91         6\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         8\n",
      "          18       1.00      1.00      1.00         6\n",
      "          19       0.86      1.00      0.92         6\n",
      "          20       1.00      1.00      1.00        10\n",
      "          21       1.00      1.00      1.00         8\n",
      "          22       1.00      0.83      0.91         6\n",
      "          23       1.00      1.00      1.00         9\n",
      "          24       1.00      0.90      0.95        10\n",
      "          25       1.00      0.71      0.83         7\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       1.00      1.00      1.00         7\n",
      "          28       1.00      1.00      1.00         9\n",
      "          29       0.88      1.00      0.93         7\n",
      "          30       1.00      1.00      1.00        10\n",
      "          31       1.00      1.00      1.00        15\n",
      "          32       1.00      1.00      1.00         8\n",
      "          33       0.90      1.00      0.95         9\n",
      "          34       1.00      1.00      1.00         8\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      0.83      0.91        12\n",
      "          37       1.00      1.00      1.00         6\n",
      "          38       1.00      1.00      1.00         7\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       0.89      0.89      0.89         9\n",
      "          41       0.90      1.00      0.95         9\n",
      "          42       1.00      1.00      1.00         8\n",
      "          43       1.00      1.00      1.00         7\n",
      "          44       1.00      1.00      1.00         8\n",
      "          45       0.89      0.89      0.89         9\n",
      "          46       1.00      1.00      1.00         8\n",
      "          47       1.00      1.00      1.00         7\n",
      "          48       1.00      1.00      1.00         9\n",
      "          49       1.00      1.00      1.00        12\n",
      "          50       1.00      1.00      1.00        10\n",
      "          51       0.90      0.90      0.90        10\n",
      "          52       1.00      1.00      1.00         9\n",
      "          53       0.92      0.92      0.92        12\n",
      "          54       0.92      1.00      0.96        11\n",
      "          55       1.00      0.86      0.92         7\n",
      "          56       1.00      1.00      1.00         6\n",
      "          57       0.75      0.75      0.75         4\n",
      "          58       1.00      0.92      0.96        12\n",
      "          59       1.00      1.00      1.00         7\n",
      "          60       1.00      0.92      0.96        12\n",
      "          61       0.50      1.00      0.67         3\n",
      "          62       0.89      1.00      0.94         8\n",
      "          63       1.00      1.00      1.00         7\n",
      "          64       0.60      0.86      0.71         7\n",
      "          65       1.00      1.00      1.00        11\n",
      "          66       1.00      1.00      1.00         9\n",
      "          67       0.89      1.00      0.94         8\n",
      "          68       1.00      1.00      1.00        10\n",
      "          69       1.00      1.00      1.00         9\n",
      "          70       1.00      0.90      0.95        10\n",
      "          71       0.71      0.56      0.63         9\n",
      "          72       1.00      1.00      1.00         5\n",
      "          73       1.00      0.86      0.92         7\n",
      "          74       0.88      1.00      0.93         7\n",
      "          75       1.00      1.00      1.00         7\n",
      "          76       1.00      0.86      0.92         7\n",
      "          77       0.75      1.00      0.86         9\n",
      "          78       1.00      1.00      1.00         8\n",
      "          79       1.00      1.00      1.00         5\n",
      "          80       0.78      0.88      0.82         8\n",
      "          81       1.00      0.91      0.95        11\n",
      "          82       1.00      1.00      1.00         6\n",
      "          83       1.00      0.83      0.91         6\n",
      "          84       1.00      0.89      0.94         9\n",
      "          85       0.89      1.00      0.94         8\n",
      "          86       0.78      0.78      0.78         9\n",
      "          87       1.00      1.00      1.00         5\n",
      "          88       0.86      1.00      0.92         6\n",
      "          89       1.00      1.00      1.00        10\n",
      "          90       0.86      0.86      0.86         7\n",
      "          91       1.00      1.00      1.00         6\n",
      "          92       1.00      1.00      1.00         7\n",
      "          93       0.80      1.00      0.89         4\n",
      "          94       1.00      1.00      1.00        15\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       770\n",
      "   macro avg       0.95      0.94      0.94       770\n",
      "weighted avg       0.95      0.94      0.94       770\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = ['precision', 'recall']\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=3,\n",
    "                       scoring='%s_macro' % score, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
