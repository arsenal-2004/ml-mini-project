{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimit\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py:2223: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "C:\\Users\\jimit\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py:2225: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "C:\\Users\\jimit\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py:2233: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] /= 2  # halve the component at -N/2\n",
      "C:\\Users\\jimit\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py:2234: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  temp = Y[sl]\n",
      "C:\\Users\\jimit\\Anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py:2236: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = temp  # set that equal to the component at -N/2\n"
     ]
    }
   ],
   "source": [
    "X, y, class_names = preprocessing.create_data_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X: (2565, 22, 57)\n",
      "Possible Classes: dict_values(['alive', 'all', 'answer', 'boy', 'building', 'buy', 'change_mind_', 'cold', 'come', 'computer_PC_', 'cost', 'crazy', 'danger', 'deaf', 'different', 'draw', 'drink', 'eat', 'exit', 'flash', 'forget', 'girl', 'give', 'glove', 'go', 'God', 'happy', 'head', 'hear', 'hello', 'his_hers', 'hot', 'how', 'hurry', 'hurt', 'I', 'innocent', 'is_true_', 'joke', 'juice', 'know', 'later', 'lose', 'love', 'make', 'man', 'maybe', 'mine', 'money', 'more', 'name', 'no', 'Norway', 'not', 'paper', 'pen', 'please', 'polite', 'question', 'read', 'ready', 'research', 'responsible', 'right', 'sad', 'same', 'science', 'share', 'shop', 'soon', 'sorry', 'spend', 'stubborn', 'surprise', 'take', 'temper', 'thank', 'think', 'tray', 'us', 'voluntary', 'wait_notyet_', 'what', 'when', 'where', 'which', 'who', 'why', 'wild', 'will', 'write', 'wrong', 'yes', 'you', 'zero'])\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of X:', X.shape)\n",
    "print('Possible Classes:', class_names.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X after flattening:  (2565, 1254)\n"
     ]
    }
   ],
   "source": [
    "X_flat = preprocessing.flatten_data(X)\n",
    "print('Dimensions of X after flattening: ', X_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions:  (1795, 1254)\n",
      "Test set dimensions:  (770, 1254)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.3)\n",
    "print('Training set dimensions: ', X_train.shape)\n",
    "print('Test set dimensions: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up parameters for GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.892 (+/-0.023) for {'C': 0.01, 'kernel': 'linear'}\n",
      "0.893 (+/-0.027) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.893 (+/-0.027) for {'C': 1, 'kernel': 'linear'}\n",
      "0.893 (+/-0.027) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88         7\n",
      "          1       1.00      1.00      1.00         4\n",
      "          2       1.00      1.00      1.00         5\n",
      "          3       0.44      0.57      0.50         7\n",
      "          4       1.00      1.00      1.00         6\n",
      "          5       0.80      0.67      0.73         6\n",
      "          6       1.00      0.86      0.92        14\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.90      0.82      0.86        11\n",
      "          9       1.00      0.83      0.91         6\n",
      "         10       0.79      1.00      0.88        11\n",
      "         11       0.70      1.00      0.82         7\n",
      "         12       0.90      0.90      0.90        10\n",
      "         13       0.89      1.00      0.94         8\n",
      "         14       1.00      1.00      1.00         6\n",
      "         15       1.00      0.89      0.94         9\n",
      "         16       1.00      1.00      1.00         7\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       1.00      1.00      1.00         9\n",
      "         19       1.00      1.00      1.00        10\n",
      "         20       1.00      1.00      1.00         9\n",
      "         21       0.60      0.60      0.60         5\n",
      "         22       0.90      1.00      0.95         9\n",
      "         23       0.67      0.86      0.75         7\n",
      "         24       1.00      1.00      1.00         8\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00        10\n",
      "         27       1.00      0.75      0.86         8\n",
      "         28       1.00      1.00      1.00         6\n",
      "         29       1.00      0.89      0.94         9\n",
      "         30       1.00      1.00      1.00        15\n",
      "         31       1.00      0.89      0.94         9\n",
      "         32       0.88      1.00      0.93         7\n",
      "         33       1.00      1.00      1.00         5\n",
      "         34       1.00      0.89      0.94         9\n",
      "         35       0.88      0.88      0.88         8\n",
      "         36       0.62      1.00      0.76         8\n",
      "         37       1.00      1.00      1.00         5\n",
      "         38       1.00      1.00      1.00         5\n",
      "         39       1.00      1.00      1.00        10\n",
      "         40       1.00      1.00      1.00        10\n",
      "         41       1.00      1.00      1.00         6\n",
      "         42       1.00      1.00      1.00         5\n",
      "         43       1.00      1.00      1.00        10\n",
      "         44       1.00      1.00      1.00         7\n",
      "         45       0.78      0.78      0.78         9\n",
      "         46       0.75      1.00      0.86         3\n",
      "         47       1.00      0.40      0.57        10\n",
      "         48       0.86      0.75      0.80         8\n",
      "         49       1.00      0.80      0.89         5\n",
      "         50       1.00      1.00      1.00        11\n",
      "         51       0.85      1.00      0.92        11\n",
      "         52       1.00      1.00      1.00         5\n",
      "         53       1.00      1.00      1.00        10\n",
      "         54       1.00      1.00      1.00        12\n",
      "         55       0.80      0.62      0.70        13\n",
      "         56       1.00      1.00      1.00         9\n",
      "         57       0.56      1.00      0.71         5\n",
      "         58       1.00      0.89      0.94         9\n",
      "         59       0.83      0.83      0.83        12\n",
      "         60       1.00      0.86      0.92         7\n",
      "         61       1.00      1.00      1.00         8\n",
      "         62       1.00      1.00      1.00         9\n",
      "         63       1.00      1.00      1.00         7\n",
      "         64       1.00      0.80      0.89        10\n",
      "         65       1.00      1.00      1.00         6\n",
      "         66       0.88      1.00      0.93         7\n",
      "         67       1.00      1.00      1.00        10\n",
      "         68       0.88      0.88      0.88         8\n",
      "         69       0.69      0.90      0.78        10\n",
      "         70       0.75      1.00      0.86         9\n",
      "         71       1.00      1.00      1.00         8\n",
      "         72       1.00      1.00      1.00         8\n",
      "         73       1.00      1.00      1.00         4\n",
      "         74       1.00      0.86      0.92         7\n",
      "         75       1.00      1.00      1.00        13\n",
      "         76       1.00      1.00      1.00         5\n",
      "         77       0.80      0.80      0.80         5\n",
      "         78       1.00      1.00      1.00         7\n",
      "         79       1.00      0.89      0.94         9\n",
      "         80       1.00      0.90      0.95        10\n",
      "         81       1.00      1.00      1.00        12\n",
      "         82       0.86      0.75      0.80         8\n",
      "         83       0.88      0.88      0.88         8\n",
      "         84       1.00      0.92      0.96        13\n",
      "         85       0.88      1.00      0.93         7\n",
      "         86       1.00      0.71      0.83         7\n",
      "         87       1.00      0.90      0.95        10\n",
      "         88       1.00      1.00      1.00         8\n",
      "         89       0.88      0.88      0.88         8\n",
      "         90       0.83      0.83      0.83        12\n",
      "         91       0.91      1.00      0.95        10\n",
      "         92       1.00      0.71      0.83         7\n",
      "         93       0.86      1.00      0.92         6\n",
      "         94       1.00      0.88      0.93         8\n",
      "\n",
      "avg / total       0.93      0.92      0.92       770\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.874 (+/-0.030) for {'C': 0.01, 'kernel': 'linear'}\n",
      "0.876 (+/-0.033) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.876 (+/-0.033) for {'C': 1, 'kernel': 'linear'}\n",
      "0.876 (+/-0.033) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88         7\n",
      "          1       1.00      1.00      1.00         4\n",
      "          2       1.00      1.00      1.00         5\n",
      "          3       0.44      0.57      0.50         7\n",
      "          4       1.00      1.00      1.00         6\n",
      "          5       0.80      0.67      0.73         6\n",
      "          6       1.00      0.86      0.92        14\n",
      "          7       1.00      1.00      1.00         6\n",
      "          8       0.90      0.82      0.86        11\n",
      "          9       1.00      0.83      0.91         6\n",
      "         10       0.79      1.00      0.88        11\n",
      "         11       0.70      1.00      0.82         7\n",
      "         12       0.90      0.90      0.90        10\n",
      "         13       0.89      1.00      0.94         8\n",
      "         14       1.00      1.00      1.00         6\n",
      "         15       1.00      0.89      0.94         9\n",
      "         16       1.00      1.00      1.00         7\n",
      "         17       1.00      1.00      1.00         6\n",
      "         18       1.00      1.00      1.00         9\n",
      "         19       1.00      1.00      1.00        10\n",
      "         20       1.00      1.00      1.00         9\n",
      "         21       0.60      0.60      0.60         5\n",
      "         22       0.90      1.00      0.95         9\n",
      "         23       0.67      0.86      0.75         7\n",
      "         24       1.00      1.00      1.00         8\n",
      "         25       1.00      1.00      1.00         2\n",
      "         26       1.00      1.00      1.00        10\n",
      "         27       1.00      0.75      0.86         8\n",
      "         28       1.00      1.00      1.00         6\n",
      "         29       1.00      0.89      0.94         9\n",
      "         30       1.00      1.00      1.00        15\n",
      "         31       1.00      0.89      0.94         9\n",
      "         32       0.88      1.00      0.93         7\n",
      "         33       1.00      1.00      1.00         5\n",
      "         34       1.00      0.89      0.94         9\n",
      "         35       0.88      0.88      0.88         8\n",
      "         36       0.62      1.00      0.76         8\n",
      "         37       1.00      1.00      1.00         5\n",
      "         38       1.00      1.00      1.00         5\n",
      "         39       1.00      1.00      1.00        10\n",
      "         40       1.00      1.00      1.00        10\n",
      "         41       1.00      1.00      1.00         6\n",
      "         42       1.00      1.00      1.00         5\n",
      "         43       1.00      1.00      1.00        10\n",
      "         44       1.00      1.00      1.00         7\n",
      "         45       0.78      0.78      0.78         9\n",
      "         46       0.75      1.00      0.86         3\n",
      "         47       1.00      0.40      0.57        10\n",
      "         48       0.86      0.75      0.80         8\n",
      "         49       1.00      0.80      0.89         5\n",
      "         50       1.00      1.00      1.00        11\n",
      "         51       0.85      1.00      0.92        11\n",
      "         52       1.00      1.00      1.00         5\n",
      "         53       1.00      1.00      1.00        10\n",
      "         54       1.00      1.00      1.00        12\n",
      "         55       0.80      0.62      0.70        13\n",
      "         56       1.00      1.00      1.00         9\n",
      "         57       0.56      1.00      0.71         5\n",
      "         58       1.00      0.89      0.94         9\n",
      "         59       0.83      0.83      0.83        12\n",
      "         60       1.00      0.86      0.92         7\n",
      "         61       1.00      1.00      1.00         8\n",
      "         62       1.00      1.00      1.00         9\n",
      "         63       1.00      1.00      1.00         7\n",
      "         64       1.00      0.80      0.89        10\n",
      "         65       1.00      1.00      1.00         6\n",
      "         66       0.88      1.00      0.93         7\n",
      "         67       1.00      1.00      1.00        10\n",
      "         68       0.88      0.88      0.88         8\n",
      "         69       0.69      0.90      0.78        10\n",
      "         70       0.75      1.00      0.86         9\n",
      "         71       1.00      1.00      1.00         8\n",
      "         72       1.00      1.00      1.00         8\n",
      "         73       1.00      1.00      1.00         4\n",
      "         74       1.00      0.86      0.92         7\n",
      "         75       1.00      1.00      1.00        13\n",
      "         76       1.00      1.00      1.00         5\n",
      "         77       0.80      0.80      0.80         5\n",
      "         78       1.00      1.00      1.00         7\n",
      "         79       1.00      0.89      0.94         9\n",
      "         80       1.00      0.90      0.95        10\n",
      "         81       1.00      1.00      1.00        12\n",
      "         82       0.86      0.75      0.80         8\n",
      "         83       0.88      0.88      0.88         8\n",
      "         84       1.00      0.92      0.96        13\n",
      "         85       0.88      1.00      0.93         7\n",
      "         86       1.00      0.71      0.83         7\n",
      "         87       1.00      0.90      0.95        10\n",
      "         88       1.00      1.00      1.00         8\n",
      "         89       0.88      0.88      0.88         8\n",
      "         90       0.83      0.83      0.83        12\n",
      "         91       0.91      1.00      0.95        10\n",
      "         92       1.00      0.71      0.83         7\n",
      "         93       0.86      1.00      0.92         6\n",
      "         94       1.00      0.88      0.93         8\n",
      "\n",
      "avg / total       0.93      0.92      0.92       770\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = ['precision', 'recall']\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=3,\n",
    "                       scoring='%s_macro' % score, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
