{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2223: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2225: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = X[sl]\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2233: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] /= 2  # halve the component at -N/2\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2234: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  temp = Y[sl]\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/scipy/signal/signaltools.py:2236: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y[sl] = temp  # set that equal to the component at -N/2\n"
     ]
    }
   ],
   "source": [
    "X, y, class_names = preprocessing.create_data_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X: (2565, 22, 57)\n",
      "Possible Classes: dict_values(['make', 'polite', 'draw', 'soon', 'money', 'cost', 'when', 'innocent', 'pen', 'name', 'know', 'paper', 'no', 'I', 'tray', 'research', 'computer_PC_', 'ready', 'God', 'what', 'wait_notyet_', 'building', 'yes', 'different', 'sad', 'man', 'right', 'later', 'all', 'hurry', 'his_hers', 'hear', 'danger', 'eat', 'drink', 'share', 'thank', 'you', 'temper', 'juice', 'hurt', 'wild', 'please', 'give', 'come', 'glove', 'forget', 'more', 'which', 'shop', 'lose', 'maybe', 'stubborn', 'question', 'where', 'sorry', 'spend', 'girl', 'Norway', 'write', 'science', 'zero', 'buy', 'happy', 'hot', 'not', 'take', 'will', 'head', 'go', 'is_true_', 'think', 'why', 'deaf', 'answer', 'surprise', 'how', 'read', 'love', 'flash', 'boy', 'voluntary', 'hello', 'cold', 'change_mind_', 'mine', 'crazy', 'responsible', 'who', 'joke', 'same', 'wrong', 'alive', 'us', 'exit'])\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of X:', X.shape)\n",
    "print('Possible Classes:', class_names.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X after flattening:  (2565, 1254)\n"
     ]
    }
   ],
   "source": [
    "X_flat = preprocessing.flatten_data(X)\n",
    "print('Dimensions of X after flattening: ', X_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions:  (1795, 1254)\n",
      "Test set dimensions:  (770, 1254)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.3)\n",
    "print('Training set dimensions: ', X_train.shape)\n",
    "print('Test set dimensions: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up parameters for GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_parameters = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10]}]\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'C': [1, 10, 100], 'gamma': [0.01, 0.1]}, {'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.9min finished\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.554 (+/-0.017) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.832 (+/-0.042) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.904 (+/-0.016) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.849 (+/-0.032) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.923 (+/-0.009) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.849 (+/-0.032) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.407 (+/-0.039) for {'C': 0.01, 'kernel': 'linear'}\n",
      "0.885 (+/-0.006) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.930 (+/-0.013) for {'C': 1, 'kernel': 'linear'}\n",
      "0.931 (+/-0.013) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.53      0.89      0.67         9\n",
      "           4       1.00      1.00      1.00         9\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.88      0.78      0.82         9\n",
      "           7       1.00      0.70      0.82        10\n",
      "           8       0.44      1.00      0.62         4\n",
      "           9       1.00      1.00      1.00         8\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       0.89      1.00      0.94         8\n",
      "          12       1.00      1.00      1.00         4\n",
      "          13       0.88      1.00      0.93         7\n",
      "          14       1.00      0.85      0.92        13\n",
      "          15       0.89      0.89      0.89         9\n",
      "          16       1.00      1.00      1.00         4\n",
      "          17       1.00      1.00      1.00         9\n",
      "          18       1.00      1.00      1.00         8\n",
      "          19       1.00      0.78      0.88         9\n",
      "          20       1.00      1.00      1.00         6\n",
      "          21       1.00      1.00      1.00        11\n",
      "          22       1.00      0.83      0.91         6\n",
      "          23       1.00      1.00      1.00         7\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       0.86      0.86      0.86         7\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         9\n",
      "          29       1.00      1.00      1.00         9\n",
      "          30       1.00      1.00      1.00         4\n",
      "          31       0.93      1.00      0.96        13\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      0.86      0.86         7\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       0.86      1.00      0.92         6\n",
      "          37       1.00      1.00      1.00         6\n",
      "          38       1.00      1.00      1.00        10\n",
      "          39       1.00      1.00      1.00         8\n",
      "          40       1.00      1.00      1.00         5\n",
      "          41       1.00      1.00      1.00        11\n",
      "          42       1.00      1.00      1.00         6\n",
      "          43       1.00      1.00      1.00         5\n",
      "          44       1.00      0.70      0.82        10\n",
      "          45       0.75      0.75      0.75         8\n",
      "          46       1.00      1.00      1.00        10\n",
      "          47       1.00      1.00      1.00         7\n",
      "          48       0.85      1.00      0.92        11\n",
      "          49       0.91      0.71      0.80        14\n",
      "          50       1.00      1.00      1.00        10\n",
      "          51       0.83      1.00      0.91         5\n",
      "          52       1.00      1.00      1.00        15\n",
      "          53       1.00      1.00      1.00         8\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       0.90      1.00      0.95         9\n",
      "          56       1.00      1.00      1.00        12\n",
      "          57       1.00      0.78      0.88         9\n",
      "          58       1.00      1.00      1.00         6\n",
      "          59       1.00      1.00      1.00        12\n",
      "          60       1.00      1.00      1.00         6\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       0.71      0.91      0.80        11\n",
      "          63       0.92      1.00      0.96        12\n",
      "          64       1.00      0.86      0.92         7\n",
      "          65       1.00      1.00      1.00         7\n",
      "          66       0.82      0.90      0.86        10\n",
      "          67       1.00      0.86      0.92         7\n",
      "          68       0.88      0.88      0.88         8\n",
      "          69       1.00      1.00      1.00         7\n",
      "          70       1.00      0.88      0.93         8\n",
      "          71       0.50      1.00      0.67         6\n",
      "          72       1.00      1.00      1.00        11\n",
      "          73       1.00      1.00      1.00         8\n",
      "          74       1.00      1.00      1.00        10\n",
      "          75       1.00      1.00      1.00         4\n",
      "          76       1.00      1.00      1.00         6\n",
      "          77       0.78      1.00      0.88         7\n",
      "          78       1.00      1.00      1.00         8\n",
      "          79       1.00      1.00      1.00         9\n",
      "          80       0.89      0.67      0.76        12\n",
      "          81       1.00      1.00      1.00        10\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       1.00      0.91      0.95        11\n",
      "          84       1.00      1.00      1.00         3\n",
      "          85       0.62      0.83      0.71         6\n",
      "          86       1.00      0.64      0.78        11\n",
      "          87       1.00      1.00      1.00        12\n",
      "          88       0.62      0.45      0.53        11\n",
      "          89       1.00      1.00      1.00         8\n",
      "          90       0.75      1.00      0.86         6\n",
      "          91       1.00      0.86      0.92         7\n",
      "          92       1.00      1.00      1.00         2\n",
      "          93       1.00      0.92      0.96        12\n",
      "          94       1.00      1.00      1.00         7\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       770\n",
      "   macro avg       0.94      0.94      0.94       770\n",
      "weighted avg       0.95      0.93      0.93       770\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.0min finished\n",
      "/home/parth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.551 (+/-0.018) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.780 (+/-0.053) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.889 (+/-0.019) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.815 (+/-0.042) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.907 (+/-0.014) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.815 (+/-0.042) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.403 (+/-0.012) for {'C': 0.01, 'kernel': 'linear'}\n",
      "0.866 (+/-0.012) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.918 (+/-0.017) for {'C': 1, 'kernel': 'linear'}\n",
      "0.917 (+/-0.017) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.54      0.78      0.64         9\n",
      "           4       1.00      1.00      1.00         9\n",
      "           5       1.00      1.00      1.00         7\n",
      "           6       0.88      0.78      0.82         9\n",
      "           7       1.00      0.80      0.89        10\n",
      "           8       0.44      1.00      0.62         4\n",
      "           9       1.00      1.00      1.00         8\n",
      "          10       1.00      1.00      1.00         7\n",
      "          11       0.89      1.00      0.94         8\n",
      "          12       1.00      1.00      1.00         4\n",
      "          13       0.88      1.00      0.93         7\n",
      "          14       1.00      0.85      0.92        13\n",
      "          15       0.89      0.89      0.89         9\n",
      "          16       1.00      1.00      1.00         4\n",
      "          17       1.00      1.00      1.00         9\n",
      "          18       1.00      1.00      1.00         8\n",
      "          19       1.00      0.78      0.88         9\n",
      "          20       1.00      1.00      1.00         6\n",
      "          21       1.00      1.00      1.00        11\n",
      "          22       1.00      0.83      0.91         6\n",
      "          23       1.00      1.00      1.00         7\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       0.86      0.86      0.86         7\n",
      "          26       1.00      1.00      1.00         8\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         9\n",
      "          29       1.00      1.00      1.00         9\n",
      "          30       1.00      1.00      1.00         4\n",
      "          31       0.93      1.00      0.96        13\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      0.86      0.86         7\n",
      "          34       1.00      1.00      1.00         3\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       0.86      1.00      0.92         6\n",
      "          37       1.00      1.00      1.00         6\n",
      "          38       1.00      1.00      1.00        10\n",
      "          39       1.00      1.00      1.00         8\n",
      "          40       1.00      1.00      1.00         5\n",
      "          41       1.00      1.00      1.00        11\n",
      "          42       1.00      1.00      1.00         6\n",
      "          43       1.00      1.00      1.00         5\n",
      "          44       1.00      0.70      0.82        10\n",
      "          45       0.75      0.75      0.75         8\n",
      "          46       1.00      1.00      1.00        10\n",
      "          47       1.00      1.00      1.00         7\n",
      "          48       0.85      1.00      0.92        11\n",
      "          49       0.91      0.71      0.80        14\n",
      "          50       1.00      1.00      1.00        10\n",
      "          51       0.83      1.00      0.91         5\n",
      "          52       1.00      1.00      1.00        15\n",
      "          53       1.00      1.00      1.00         8\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       0.90      1.00      0.95         9\n",
      "          56       1.00      1.00      1.00        12\n",
      "          57       1.00      0.78      0.88         9\n",
      "          58       1.00      1.00      1.00         6\n",
      "          59       1.00      1.00      1.00        12\n",
      "          60       1.00      1.00      1.00         6\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       0.71      0.91      0.80        11\n",
      "          63       0.92      1.00      0.96        12\n",
      "          64       1.00      0.86      0.92         7\n",
      "          65       1.00      1.00      1.00         7\n",
      "          66       0.82      0.90      0.86        10\n",
      "          67       1.00      0.86      0.92         7\n",
      "          68       0.88      0.88      0.88         8\n",
      "          69       1.00      1.00      1.00         7\n",
      "          70       1.00      0.88      0.93         8\n",
      "          71       0.50      1.00      0.67         6\n",
      "          72       1.00      1.00      1.00        11\n",
      "          73       1.00      1.00      1.00         8\n",
      "          74       1.00      1.00      1.00        10\n",
      "          75       1.00      1.00      1.00         4\n",
      "          76       1.00      1.00      1.00         6\n",
      "          77       0.78      1.00      0.88         7\n",
      "          78       1.00      1.00      1.00         8\n",
      "          79       1.00      1.00      1.00         9\n",
      "          80       0.89      0.67      0.76        12\n",
      "          81       1.00      1.00      1.00        10\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       1.00      0.91      0.95        11\n",
      "          84       1.00      1.00      1.00         3\n",
      "          85       0.71      0.83      0.77         6\n",
      "          86       1.00      0.64      0.78        11\n",
      "          87       1.00      1.00      1.00        12\n",
      "          88       0.60      0.55      0.57        11\n",
      "          89       1.00      1.00      1.00         8\n",
      "          90       0.75      1.00      0.86         6\n",
      "          91       1.00      0.86      0.92         7\n",
      "          92       1.00      1.00      1.00         2\n",
      "          93       1.00      0.92      0.96        12\n",
      "          94       1.00      1.00      1.00         7\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       770\n",
      "   macro avg       0.95      0.94      0.94       770\n",
      "weighted avg       0.95      0.93      0.94       770\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = ['precision', 'recall']\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=3,\n",
    "                       scoring='%s_macro' % score, n_jobs=-1, verbose=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
